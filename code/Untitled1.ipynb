{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Training-AI-Agents\" data-toc-modified-id=\"Training-AI-Agents-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Training AI Agents</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Imports</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stable-Baselines-Imports\" data-toc-modified-id=\"Stable-Baselines-Imports-1.0.1.1\"><span class=\"toc-item-num\">1.0.1.1&nbsp;&nbsp;</span>Stable Baselines Imports</a></span></li></ul></li><li><span><a href=\"#Creating-the-Player-Class\" data-toc-modified-id=\"Creating-the-Player-Class-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Creating the Player Class</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reading-in-shot_distr.csv\" data-toc-modified-id=\"Reading-in-shot_distr.csv-1.0.2.1\"><span class=\"toc-item-num\">1.0.2.1&nbsp;&nbsp;</span>Reading in shot_distr.csv</a></span></li></ul></li><li><span><a href=\"#Creating-a-custom-Environment\" data-toc-modified-id=\"Creating-a-custom-Environment-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>Creating a custom Environment</a></span></li><li><span><a href=\"#Checking-the-environment-using-Stable-Baselines\" data-toc-modified-id=\"Checking-the-environment-using-Stable-Baselines-1.0.4\"><span class=\"toc-item-num\">1.0.4&nbsp;&nbsp;</span>Checking the environment using Stable Baselines</a></span></li><li><span><a href=\"#Random-Agent-Decisions\" data-toc-modified-id=\"Random-Agent-Decisions-1.0.5\"><span class=\"toc-item-num\">1.0.5&nbsp;&nbsp;</span>Random Agent Decisions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis:\" data-toc-modified-id=\"Analysis:-1.0.5.1\"><span class=\"toc-item-num\">1.0.5.1&nbsp;&nbsp;</span>Analysis:</a></span></li><li><span><a href=\"#Analysis:\" data-toc-modified-id=\"Analysis:-1.0.5.2\"><span class=\"toc-item-num\">1.0.5.2&nbsp;&nbsp;</span>Analysis:</a></span></li></ul></li><li><span><a href=\"#Q-Learning\" data-toc-modified-id=\"Q-Learning-1.0.6\"><span class=\"toc-item-num\">1.0.6&nbsp;&nbsp;</span>Q Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis-of-Q-Learning\" data-toc-modified-id=\"Analysis-of-Q-Learning-1.0.6.1\"><span class=\"toc-item-num\">1.0.6.1&nbsp;&nbsp;</span>Analysis of Q-Learning</a></span></li></ul></li><li><span><a href=\"#PPO---Promixal-Policy-Optimization\" data-toc-modified-id=\"PPO---Promixal-Policy-Optimization-1.0.7\"><span class=\"toc-item-num\">1.0.7&nbsp;&nbsp;</span>PPO - Promixal Policy Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis:\" data-toc-modified-id=\"Analysis:-1.0.7.1\"><span class=\"toc-item-num\">1.0.7.1&nbsp;&nbsp;</span>Analysis:</a></span></li></ul></li><li><span><a href=\"#PPO2---Promixal-Policy-Optimization\" data-toc-modified-id=\"PPO2---Promixal-Policy-Optimization-1.0.8\"><span class=\"toc-item-num\">1.0.8&nbsp;&nbsp;</span>PPO2 - Promixal Policy Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis:\" data-toc-modified-id=\"Analysis:-1.0.8.1\"><span class=\"toc-item-num\">1.0.8.1&nbsp;&nbsp;</span>Analysis:</a></span></li></ul></li><li><span><a href=\"#A2C---Advantage-Actor-Critic\" data-toc-modified-id=\"A2C---Advantage-Actor-Critic-1.0.9\"><span class=\"toc-item-num\">1.0.9&nbsp;&nbsp;</span>A2C - Advantage Actor Critic</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis:\" data-toc-modified-id=\"Analysis:-1.0.9.1\"><span class=\"toc-item-num\">1.0.9.1&nbsp;&nbsp;</span>Analysis:</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training AI Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "I am using Stable Baselines, and Open AI's gym to create and test the environment. I am also brining in player.py, that will provide the probabilities of an action happening, which are determined from NBA player statistics from the current NBA season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lots of help from Dustin Pierce at General Assembly\n",
    "#https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html\n",
    "#https://github.com/koulanurag/ma-gym/blob/master/ma_gym/envs/pong_duel/pong_duel.py\n",
    "#https://github.com/hardmaru/slimevolleygym/blob/master/slimevolleygym/slimevolley.py\n",
    "#https://medium.com/@m.alzantot/you-can-see-what-is-the-observation-space-by-print-env-observation-space-c4e59e64ac52\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import random\n",
    "import time\n",
    "# import ball  --potential future add on\n",
    "# from player import Player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stable Baselines Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.env_checker import check_env\n",
    "\n",
    "# from stable_baselines.common.policies import MlpPolicy\n",
    "# from stable_baselines.common import make_vec_env\n",
    "# from stable_baselines import PPO1, A2C, PPO2, results_plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Player Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in shot_distr.csv\n",
    "\n",
    "I am sampling from the distributions created in the previous workbook to use as shooting percentages for the players on offense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/shot_distr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLAYER_NAME</th>\n",
       "      <th>3pa</th>\n",
       "      <th>3pb</th>\n",
       "      <th>fga</th>\n",
       "      <th>fgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kristaps Porzingis</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Luka Doncic</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RJ Barrett</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Julius Randle</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PLAYER_NAME  3pa  3pb   fga   fgb\n",
       "0  Kristaps Porzingis  6.0  8.0   8.0  13.0\n",
       "1         Luka Doncic  2.0  9.0  13.0  18.0\n",
       "2          RJ Barrett  5.0  3.0  13.0   7.0\n",
       "3       Julius Randle  2.0  5.0   9.0  11.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    \n",
    "    def __init__(self, player_name, _has_ball=False):\n",
    "\n",
    "        self.player_name = player_name\n",
    "        self._is_defended = False\n",
    "        self._has_ball = True\n",
    "        self._close_range = False\n",
    "        self._midrange = False\n",
    "        self._three_point_range = True\n",
    "\n",
    "        self.shooting_close = np.random.beta(df['fga'][df['PLAYER_NAME'] == self.player_name], df['fgb'][df['PLAYER_NAME'] == self.player_name])\n",
    "        self.shooing_midrange = np.random.beta(df['fga'][df['PLAYER_NAME'] == self.player_name], df['fgb'][df['PLAYER_NAME'] == self.player_name])\n",
    "        self.shooting3pts = np.random.beta(df['3pa'][df['PLAYER_NAME'] == self.player_name], df['3pb'][df['PLAYER_NAME'] == self.player_name])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A player on defense for this simplified game would not need attributes for shooting, because as soon as the defense gets the ball the round is over and the environment is reset. They will have attributes to determine the probability of a steal and block instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayerD():\n",
    "\n",
    "    def __init__(self, player_name):\n",
    "        \n",
    "        self.player_name = player_name\n",
    "        self._is_defended = False\n",
    "        self._has_ball = False\n",
    "        self._close_range = False\n",
    "        self._midrange = False\n",
    "        self._three_point_range = True\n",
    "\n",
    "        self.steal = np.random.beta(1, 25)\n",
    "        self.block = np.random.beta(1, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasketballEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where multiple agents learn strategies to put the ball in the hoop.\n",
    "    For this simple iteration, actions will be determined by probabilities rather than physics.\n",
    "    \"\"\"\n",
    "    # In google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "\n",
    "    \n",
    "    def __init__(self, step_cost=0.0, reward=0, max_rounds=1):\n",
    "        #Grid size will be standard basketball halfcourt at 6\"=1'-0\" scale\n",
    "        self._grid_shape = (100, 94)\n",
    "\n",
    "        #Number of players\n",
    "        self.n_agents = 4\n",
    "        self.n_teams = 2\n",
    "        self.n_agents_team_A = int(self.n_agents / 2)\n",
    "        self.n_agents_team_B = int(self.n_agents / 2)\n",
    "        self.reward = reward\n",
    "        self._max_rounds = max_rounds\n",
    "        self.action_space = spaces.MultiDiscrete([9, 2, 2])\n",
    "        self.player_w_ball=[True, False]\n",
    "\n",
    "        self._step_count = 0\n",
    "        self._step_cost = step_cost\n",
    "        self._total_episode_reward = [0 for _ in range(self.n_teams)]\n",
    "        self.agent_pos = {_: None for _ in range(self.n_agents)}\n",
    "#         self.x = [self.agent_pos[x][0] for x in range(0,4)]\n",
    "#         self.y = [self.agent_pos[y][1] for y in range(0,4)]\n",
    "        self.t = 24\n",
    "\n",
    "        self._agent_dones = None\n",
    "        self._rounds = 0\n",
    "\n",
    "        # Observing agent positions for 4 agents\n",
    "        self._obs_low = np.array([0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "        self._obs_high = np.array([1., 1., 1., 1., 1., 1., 1., 1,])\n",
    "        self.observation_space = spaces.Box(low=self._obs_low, high=self._obs_high,\n",
    "                                        dtype=np.float32)\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "        \n",
    "    def is_done(self):\n",
    "        if self._rounds >= self._max_rounds:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_action_meanings(self, agent_i=None):\n",
    "        if agent_i is not None:\n",
    "            assert agent_i <= self.n_agents\n",
    "            return [ACTION_MEANING[i] for i in range(self.action_space[agent_i].n)]\n",
    "        else:\n",
    "            return [[ACTION_MEANING[i] for i in range(ac.n)] for ac in self.action_space]\n",
    "\n",
    "    def __create_grid(self):\n",
    "        _grid = [[PRE_IDS['empty'] for _ in range(self._grid_shape[1])] for row in range(self._grid_shape[0])]\n",
    "        return _grid\n",
    "\n",
    "    def __update_agent_view(self, agent_i):\n",
    "        for row in range(self.agent_prev_pos[agent_i][0],\n",
    "                         self.agent_prev_pos[agent_i][0]):\n",
    "            self._full_obs[row][self.agent_prev_pos[agent_i][1]] = PRE_IDS['empty']\n",
    "\n",
    "        for row in range(self.agent_pos[agent_i][0], self.agent_pos[agent_i][0]):\n",
    "            self._full_obs[row][self.agent_pos[agent_i][1]] = PRE_IDS['agent'] + str(agent_i + 1) \\\n",
    "                                                              + '_' + str(row - self.agent_pos[agent_i][0])\n",
    "\n",
    "#     def __draw_base_img(self):\n",
    "#         self._base_img = draw.draw_grid(self._grid_shape[0], self._grid_shape[1],\n",
    "#                                    cell_size=CELL_SIZE, fill='white', line_color='white')\n",
    "\n",
    "    def __init_full_obs(self):\n",
    "        self._full_obs = self.__create_grid()\n",
    "        for agent_i in range(self.n_agents):\n",
    "            self.__update_agent_view(agent_i)\n",
    "\n",
    "        for agent_i in range(self.n_agents):\n",
    "            self.__update_agent_view(agent_i)\n",
    "\n",
    "#         self.__draw_base_img()\n",
    "\n",
    "    #Countdown timer as 24 second shot clock for each round\n",
    "    #https://www.geeksforgeeks.org/how-to-create-a-countdown-timer-using-python/\n",
    "    def countdown(self, t=24):     \n",
    "        while t: \n",
    "            mins, secs = divmod(t, 60) \n",
    "            timer = '{:02d}:{:02d}'.format(mins, secs)  \n",
    "            time.sleep(1) \n",
    "            t -= 1\n",
    "            self.t = t\n",
    "    \n",
    "    def __init_countdown(self, t):\n",
    "        return self.countdown(t)\n",
    "        \n",
    "\n",
    "    def get_agent_obs(self):\n",
    "        _obs = []\n",
    "\n",
    "        for agent_i in range(self.n_agents):\n",
    "            pos = self.agent_pos[agent_i]\n",
    "            _agent_i_obs_a = pos[0] / self._grid_shape[0]\n",
    "            _agent_i_obs_b = pos[1] / self._grid_shape[1]\n",
    "            \n",
    "            _obs.append(_agent_i_obs_a)\n",
    "            _obs.append(_agent_i_obs_b)\n",
    "\n",
    "        return np.array(_obs)\n",
    "    \n",
    "    def get_state_num(self):\n",
    "        for agent_i in range(self.n_agents):\n",
    "            return self.agent_pos[agent_i][0]*self._grid_shape[0] + self.agent_pos[agent_i][1]\n",
    "\n",
    "    def get_pos_from_state_num(self, state_num):\n",
    "        return (state_num // self._grid_shape[0], state_num % self._grid_shape[0])\n",
    "    \n",
    "##############\n",
    "#Define Reset#   \n",
    "##############\n",
    "\n",
    "    def reset(self):\n",
    "        self._rounds = 0\n",
    "        self.__init_countdown(24)\n",
    "        \n",
    "        #Set starting positions for agents in Team A\n",
    "        self.agent_pos[0] = [self._grid_shape[0]//2, self._grid_shape[1] - 2]\n",
    "        self.agent_pos[1] = [self._grid_shape[0]//5, self._grid_shape[1] - 2]\n",
    "\n",
    "        \n",
    "        #Set starting positions for agents in Team B\n",
    "        self.agent_pos[2] = [self._grid_shape[0]//2, self._grid_shape[1] - 8]\n",
    "        self.agent_pos[3] = [self._grid_shape[0]//5, self._grid_shape[1] - 8]\n",
    "\n",
    "        \n",
    "        self.agent_prev_pos = {_: self.agent_pos[_] for _ in range(self.n_agents)}\n",
    "        self._agent_dones = False\n",
    "        self.__init_full_obs()\n",
    "        self._step_count = 0\n",
    "        self._total_episode_reward = [0 for _ in range(self.n_teams)]\n",
    "\n",
    "        return self.get_agent_obs()\n",
    "\n",
    "    \n",
    "###############################\n",
    "#Define Properties and Actions#   \n",
    "###############################\n",
    "    \n",
    "            \n",
    "    #This will determine success of an action\n",
    "    def action_success(self, p_1):\n",
    "        return np.random.choice([0, 1], p=[1 - p_1, p_1])\n",
    "\n",
    "    #Determine if a player is close to the goal\n",
    "    def close_range(self):\n",
    "        for agent_i in range(n_agents):\n",
    "            if self.pos[agent_i][1] > 0 and self.pos[agent_i][1] < self._grid_shape[1] and self.pos[agent_i][0] > 0 and self.pos[agent_i][0] < self._grid_shape[0]:\n",
    "                if np.sqrt((self.pos[agent_i][0] - GOAL[0])**2 + (self.pos[agent_i][1]-GOAL[1])**2) <= 6:\n",
    "                    agent_i.player._close_range = True\n",
    "\n",
    "    #Determine if a player is mid-range from the goal\n",
    "    def midrange(self):\n",
    "        for agent_i in range(n_agents):\n",
    "            if self.pos[agent_i][1] > 0 and self.pos[agent_i][1] < self._grid_shape[1] and self.pos[agent_i][0] > 0 and self.pos[agent_i][0] < self._grid_shape[0]:\n",
    "                if player._close_range == False and player._three_point_range == False:\n",
    "                    agent_i.player._midrange = True\n",
    "\n",
    "    #Determine if a player is in three point range\n",
    "    def _three_point_range(self):\n",
    "        for agent_i in range(n_agents):\n",
    "            if self.pos[agent_i][1] > 0 and self.pos[agent_i][1] < self._grid_shape[1] and self.pos[agent_i][0] > 0 and self.pos[agent_i][0] < self._grid_shape[0]:\n",
    "                if self.pos[agent_i][1] <= 19.67 and self.pos[agent_i][0] <= 6.67 or self.pos[agent_i][1] >= 93.33:\n",
    "                    player._three_point_range = True\n",
    "                elif self.pos[agent_i][1] > 19.67 and np.sqrt((self.pos[agent_i][0] - GOAL[0])**2 + (self.pos[agent_i][1]-GOAL[1])**2) > 44.3:\n",
    "                    player._three_point_range = True\n",
    "\n",
    "    #define defensive rebound, will be a reward for the defensive team\n",
    "    def d_rebound(self):\n",
    "        return True\n",
    "        \n",
    "    def rebound(self):\n",
    "        for agent_i in range(self.n_agents_team_A):\n",
    "            if self.action_success(0.3):\n",
    "                #UPDATE THE PLAYER_W_BALL:\n",
    "#                 o_rebounder = random.choice([[True, False], [False, True]])\n",
    "#                 self.player_w_ball = o_rebounder\n",
    "#                 self.__init_countdown(14)\n",
    "#                 if o_rebounder == [True, False]:\n",
    "#                     player._has_ball = True\n",
    "#                     player2._has_ball = False\n",
    "#                 else:\n",
    "#                     player._has_ball = False\n",
    "#                     player2._has_ball = True\n",
    "                return 0.2\n",
    "            else:\n",
    "                return self.d_rebound()\n",
    "\n",
    "    #define shot, a made shot will be a reward for the offensive team\n",
    "    def shot(self):\n",
    "        \n",
    "        for agent_i in range(self.n_agents_team_A):\n",
    "\n",
    "            #Can only shoot if the player has the ball -- [1,0]\n",
    "            if player._has_ball:\n",
    "\n",
    "                #Close range shot\n",
    "                if player._close_range:\n",
    "                    shot = self.action_success(player.shooting_close)\n",
    "                    if shot == 1:\n",
    "                        return 2\n",
    "                    else:\n",
    "                        return self.rebound()\n",
    "\n",
    "                #Midrange shot\n",
    "                if player._midrange:\n",
    "                    shot = self.action_success(player.shooting_midrange)\n",
    "                    if shot == 1:\n",
    "                        return 2\n",
    "                    else:\n",
    "                        return self.rebound()\n",
    "\n",
    "                #3 point shot\n",
    "                if player._three_point_range:\n",
    "                    shot = self.action_success(player.shooting3pts)\n",
    "                    if shot == 1:\n",
    "                        return 3\n",
    "                    else:\n",
    "                        return self.rebound()\n",
    "                    \n",
    "            #Can only shoot if the player has the ball -- [0,1]\n",
    "            if player2._has_ball:\n",
    "\n",
    "                #Close range shot\n",
    "                if player2._close_range:\n",
    "                    shot = self.action_success(player2.shooting_close)\n",
    "                    if shot == 1:\n",
    "                        return 2\n",
    "                    else:\n",
    "                        return self.rebound()\n",
    "\n",
    "                #Midrange shot\n",
    "                if player2._midrange:\n",
    "                    shot = self.action_success(player2.shooting_midrange)\n",
    "                    if shot == 1:\n",
    "                        return 2\n",
    "                    else:\n",
    "                        return self.rebound()\n",
    "\n",
    "                #3 point shot\n",
    "                if player2._three_point_range:\n",
    "                    shot = self.action_success(player2.shooting3pts)\n",
    "                    if shot == 1:\n",
    "                        return 3\n",
    "                    else:\n",
    "                        return self.rebound()\n",
    "                        \n",
    "    def ball_pass(self):\n",
    "        for agent_i in range(self.n_agents_team_A):\n",
    "            if player._has_ball == True:\n",
    "                player._has_ball = False\n",
    "                player2._has_ball = True\n",
    "                self.player_w_ball = [0,1]\n",
    "                \n",
    "            if player2._has_ball == True:\n",
    "                player._has_ball = True\n",
    "                player2._has_ball = False\n",
    "                self.player_w_ball = [1,0]\n",
    "                \n",
    "            \n",
    "\n",
    "    def defended(self):\n",
    "        for agent_i in range(n_agents_team_B):\n",
    "            for agent_j in n_agents_team_A:\n",
    "                if np.sqrt(self.pos[agent_i][0]**2 + self.pos[agent_i][1]**2) < 5:\n",
    "                    agent_j.player._is_defended=True\n",
    "                else:\n",
    "                    agent_j.player._is_defended=False\n",
    "    \n",
    "    def steal(self):\n",
    "        for agent_i in range(self.n_agents_team_B):\n",
    "            for agent_j in range(self.n_agents_team_A):\n",
    "                if player._has_ball and player._is_defended:\n",
    "                    return action_success(0.02)\n",
    "    \n",
    "    def block(self):\n",
    "        for agent_i in range(self.n_agents_team_B):\n",
    "            for agent_j in range(self.n_agents_team_A):\n",
    "                if player._has_ball and player._is_defended and player._close_range:\n",
    "                    return action_success(0.04)\n",
    "                if player._has_ball and player._is_defended and player._midrange:\n",
    "                    return action_success(0.03)\n",
    "                if player._has_ball and player._is_defended and player._three_point_range:\n",
    "                    return action_success(0.02)\n",
    "                \n",
    "    def out_of_bounds(self):\n",
    "        for agent_x in range(self.n_agents):\n",
    "            if self.agent_pos[agent_x][0] > 0 and self.agent_pos[agent_x][0] < self._grid_shape[0] and self.agent_pos[agent_x][1] > 0 and self.agent_pos[agent_x][1] < self._grid_shape[1]:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "    def is_valid(self, pos):\n",
    "        return (0 <= pos[0] < self._grid_shape[0]) and (0 <= pos[1] < self._grid_shape[1])\n",
    "\n",
    "    def _is_cell_vacant(self, pos):\n",
    "        return self.is_valid(pos) and (self._full_obs[pos[0]][pos[1]] == PRE_IDS['empty'])\n",
    "\n",
    "    \n",
    "    \n",
    "###############\n",
    "#Define Render#   \n",
    "###############\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        img = copy.copy(self._base_img)\n",
    "        for agent_i in range(self.n_agents):\n",
    "            for row in self.agent_pos[agent_i][0]:\n",
    "                fill_cell(img, (row, self.agent_pos[agent_i][1]), cell_size=CELL_SIZE, fill=AGENT_COLORS[agent_i])\n",
    "\n",
    "        img = draw_border(img, border_width=2, fill='gray')\n",
    "\n",
    "        img = np.asarray(img)\n",
    "        if mode == 'rgb_array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            from gym.envs.classic_control import rendering\n",
    "            if self.viewer is None:\n",
    "                self.viewer = rendering.SimpleImageViewer()\n",
    "            self.viewer.imshow(img)\n",
    "            return self.viewer.isopen\n",
    "\n",
    "    def __update_agent_pos(self, agent_i, move):\n",
    "\n",
    "        curr_pos = copy.copy(self.agent_pos[agent_i])\n",
    "        for agent_i in range(self.n_agents):\n",
    "\n",
    "            if move == 0:  # noop\n",
    "                next_pos = None\n",
    "            elif move == 1:  # up\n",
    "                next_pos = [curr_pos[0] - 1, curr_pos[1]]\n",
    "            elif move == 2:  # upright\n",
    "                next_pos = [curr_pos[0] - 1, curr_pos[1] + 1]\n",
    "            elif move == 3:  # right\n",
    "                next_pos = [curr_pos[0], curr_pos[1] + 1]\n",
    "            elif move == 4:  # downright\n",
    "                next_pos = [curr_pos[0] + 1, curr_pos[1] + 1]\n",
    "            elif move == 5:  # down\n",
    "                next_pos = [curr_pos[0] + 1, curr_pos[1]]\n",
    "            elif move == 6:  # downleft\n",
    "                next_pos = [curr_pos[0] + 1, curr_pos[1] - 1]\n",
    "            elif move == 7:  # left\n",
    "                next_pos = [curr_pos[0], curr_pos[1] - 1]\n",
    "            elif move == 8:  # upleft\n",
    "                next_pos = [curr_pos[0] - 1, curr_pos[1] - 1]\n",
    "#                 else:\n",
    "#                     raise Exception('Action Not found!')\n",
    "\n",
    "\n",
    "            if next_pos is not None:\n",
    "                self.agent_prev_pos[agent_i] = self.agent_pos[agent_i]\n",
    "                self.agent_pos[agent_i] = next_pos\n",
    "                self.__update_agent_view(agent_i)\n",
    "            \n",
    "\n",
    "#############\n",
    "#Define Seed#   \n",
    "#############\n",
    "\n",
    "    def seed(self, n=None):\n",
    "        self.np_random, seed = seeding.np_random(n)\n",
    "        return [seed]\n",
    "\n",
    "#############\n",
    "#Define Step#   \n",
    "#############\n",
    "    \n",
    "    def step(self, action_n=[0,0,0,0]):\n",
    "#         assert len(action_n) == self.n_agents\n",
    "        self._step_count += 1\n",
    "        rewards = [self._step_cost for _ in range(self.n_teams)]\n",
    "        \n",
    "        for agent, action in enumerate(action_n):\n",
    "            \n",
    "            if action == \"UP\" and self.agent_pos[agent][1] > 0:\n",
    "                self.agent_pos[agent][1] -= 1\n",
    "            elif action == \"UPLEFT\" and self.agent_pos[agent][1] > 0 and self.agent_pos[agent][0] > 0:\n",
    "                self.agent_pos[agent][1] -= 1\n",
    "                self.agent_pos[agent][0] -= 1\n",
    "            elif action == \"LEFT\" and self.agent_pos[agent][0] > 0:\n",
    "                self.agent_pos[agent][0] -= 1\n",
    "            elif action == \"DOWNLEFT\" and self.agent_pos[agent][0] > 0 and self.agent_pos[agent][1] < self._grid_shape[1]-1:\n",
    "                self.agent_pos[agent][0] -= 1\n",
    "                self.agent_pos[agent][1] += 1\n",
    "            elif action == \"DOWN\" and self.agent_pos[agent][1] < self._grid_shape[1]-1:\n",
    "                self.agent_pos[agent][1] += 1\n",
    "            elif action == \"DOWNRIGHT\" and self.agent_pos[agent][1] < self._grid_shape[1]-1  and self.agent_pos[agent][0] < self._grid_shape[0]-1:\n",
    "                self.agent_pos[agent][1] += 1\n",
    "            elif action == \"RIGHT\" and self.agent_pos[agent][0] < self._grid_shape[0]-1:\n",
    "                self.agent_pos[agent][0] += 1\n",
    "            elif action == \"UPRIGHT\" and self.agent_pos[agent][1] > 0 and self.agent_pos[agent][0] < self._grid_shape[0]-1:\n",
    "                self.agent_pos[agent][1] -= 1\n",
    "                self.agent_pos[agent][0] += 1\n",
    "\n",
    "            if action_n == 'SHOT':\n",
    "                # if shot made, new round\n",
    "                if self.shot() == 2:\n",
    "                    rewards = [2, 0]\n",
    "                    self._rounds += 1\n",
    "\n",
    "                elif self.shot() == 3:\n",
    "                    rewards = [3, 0]\n",
    "                    self._rounds += 1\n",
    "\n",
    "                elif self.shot() == 1:\n",
    "                    rewards = [0, 1]\n",
    "                    self._rounds += 1\n",
    "\n",
    "                elif self.shot() == 0.2:\n",
    "                    rewards = [0.2, 0]\n",
    "                    self._rounds += 1\n",
    "\n",
    "            if action_n == 'BALL_PASS':\n",
    "                self.ball_pass()\n",
    "\n",
    "            if action_n == 'STEAL':\n",
    "                # if steal made, new round\n",
    "                if self.steal():\n",
    "                    rewards = [0, 2]\n",
    "                    self._rounds += 1\n",
    "\n",
    "            if action_n == 'BLOCK':\n",
    "                # if block made, new round\n",
    "                if self.block():\n",
    "                    rewards = [0, 2]\n",
    "                    self._rounds += 1\n",
    "            \n",
    "        # if Offense fails to get off a shot within time limit, new round\n",
    "        if self.t < 1:\n",
    "            rewards = [0, 2]\n",
    "            self._rounds += 1\n",
    "            \n",
    "        if self.out_of_bounds():\n",
    "            rewards = [-10, -10]\n",
    "            self._rounds +=1\n",
    "                        \n",
    "        if self._rounds == self._max_rounds:\n",
    "            self._agent_dones = True\n",
    "            \n",
    "        elif self.t < 1:\n",
    "            self._agent_dones = True\n",
    "            self._rounds +=1\n",
    "            \n",
    "        else:\n",
    "            for agent_i in range(self.n_agents_team_A):\n",
    "                self.__update_agent_pos(agent_i, action_n[agent_i])\n",
    "            for agent_j in range(self.n_agents_team_B):\n",
    "                self.__update_agent_pos(agent_j, action_n[agent_j])\n",
    "                \n",
    "        for i in range(self.n_teams):\n",
    "            self._total_episode_reward[i] += rewards[i]\n",
    "\n",
    "        return self.get_agent_obs(), rewards[0]-rewards[1], self._agent_dones, {'rounds': self._rounds}\n",
    "    \n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "# Define constants for clearer code\n",
    "\n",
    "CELL_SIZE = 5\n",
    "\n",
    "#Goal Location\n",
    "GOAL = [50, 10.5]\n",
    "\n",
    "ACTION_MEANING = {\n",
    "    0 : 'NOOP',\n",
    "    1 : 'UP',\n",
    "    2 : 'UPRIGHT',\n",
    "    3 : 'RIGHT',\n",
    "    4 : 'DOWNRIGHT',\n",
    "    5 : 'DOWN',\n",
    "    6 : 'DOWNLEFT',\n",
    "    7 : 'LEFT',\n",
    "    8 : 'UPLEFT',\n",
    "    9 : 'BALL_PASS',\n",
    "    10 : 'SHOOT',\n",
    "}\n",
    "\n",
    "AGENT_TEAMS = {\n",
    "    0: 'A',\n",
    "    1: 'A',\n",
    "    2: 'B',\n",
    "    3: 'B',\n",
    "}\n",
    "\n",
    "AGENT_COLORS = {\n",
    "    0: 'red',\n",
    "    1: 'red',\n",
    "    2: 'blue',\n",
    "    3: 'blue',\n",
    "}\n",
    "\n",
    "WALL_COLOR = 'black'\n",
    "\n",
    "# each pre-id should be unique and single char\n",
    "PRE_IDS = {\n",
    "    'agent': 'A',\n",
    "    'goal' : 'G',\n",
    "    'empty': 'O'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the environment using Stable Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = BasketballEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgentA:\n",
    "    def __init__(self, player1, _has_ball):\n",
    "        self.total_reward = [0,0, 0, 0]\n",
    "        player = Player(player1, _has_ball)\n",
    "\n",
    "    def step(self, env):\n",
    "        # current_obs = env.get_observation()\n",
    "        actions = ACTION_MEANING\n",
    "        action = [random.choice(actions), random.choice(actions), random.choice(actions), random.choice(actions)]\n",
    "        self.total_reward = env.step(action)\n",
    "\n",
    "        print(f\"Took action {action} and got reward {self.total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kendr\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.9787234 , 0.2       , 0.9787234 , 0.5       ,\n",
       "       0.91489362, 0.2       , 0.91489362])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = BasketballEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "agentA0 = RandomAgentA(player1='Luka Doncic', _has_ball=True)\n",
    "# agentA1 = RandomAgentA('Kristaps Porzingis', _has_ball=False)\n",
    "# agentB2 = RandomAgentB('Julius Randle')\n",
    "# agentB2 = RandomAgentB('RJ Barrett')\n",
    "\n",
    "count = 0\n",
    "while not env.is_done() and env._rounds==1:\n",
    "    agentA0.step(env)\n",
    "    count += 1\n",
    "print(f\"Total reward: {agentA0.total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "The reward function for the random agent loop isn't clear so I will step through random actions in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.49      , 0.95744681, 0.19      , 0.9787234 , 0.5       ,\n",
       "        0.89361702, 0.2       , 0.91489362]),\n",
       " -2,\n",
       " True,\n",
       " {'rounds': 3})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([random.choice(ACTION_MEANING),random.choice(ACTION_MEANING),random.choice(ACTION_MEANING),random.choice(ACTION_MEANING)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "The environment is working. The agents are not in their initial positions and the reward was given to the defense for a successful stop (preventing the offense from scoring.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount for future rewards\n",
    "gamma = 0.9\n",
    "\n",
    "# the learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# exploration-exploitation tradeoff: proportion of time to take a random action\n",
    "epsilon = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-10745918dc0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBasketballEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-911c1c5d0d7e>\u001b[0m in \u001b[0;36mget_state_num\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_state_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0magent_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grid_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_pos_from_state_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "env = BasketballEnv()\n",
    "env.reset()\n",
    "episodes = 10\n",
    "\n",
    "# Q(s,a): \"quality\" of taking action a in state s\n",
    "num_states = env._grid_shape[0]*env._grid_shape[1]\n",
    "num_actions = len(ACTION_MEANING)\n",
    "q_table = np.zeros([num_states, num_actions])\n",
    "\n",
    "for _ in range(0,episodes):\n",
    "    env = BasketballEnv()\n",
    "    env.reset()\n",
    "    state = env.get_state_num()\n",
    "\n",
    "    while not env.is_done():\n",
    "        # epsilon-greedy policy\n",
    "        actions = ACTION_MEANING\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action_index = np.random.choice(np.flatnonzero(q_table[state] == q_table[state].max()))\n",
    "            action = actions[action_index]\n",
    "\n",
    "        reward = env.step(action)\n",
    "        state2 = env.get_state_num()\n",
    "\n",
    "        # Q-update\n",
    "        old_q = q_table[state,action_index]\n",
    "        max_q2 = np.max(q_table[state2])\n",
    "        q_table[state,action_index] = old_q + alpha*(reward + gamma*max_q2 - old_q)\n",
    "\n",
    "        #print(f\"s={state}, a={action}, s'={state2}, r={reward}, old_q={old_q}, new_q={q_table[state,action_index]}\")\n",
    "\n",
    "        state = state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO - Promixal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BasketballEnv')\n",
    "\n",
    "model = PPO1(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo1_Basketball\")\n",
    "\n",
    "# model = PPO1.load(\"ppo1_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_baselines.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "Analysis of the PPO Model results go here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO2 - Promixal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocess environment\n",
    "env = make_vec_env('BasketballEnv', n_envs=4)\n",
    "\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo2_basketball\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO2.load(\"ppo2_basketball\")\n",
    "\n",
    "# Enjoy trained agent\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_baselines.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "Analysis of the PPO2 Model results go here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C - Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stable-baselines.readthedocs.io/en/master/modules/a2c.html\n",
    "# Parallel environments\n",
    "env = make_vec_env('BasketballEnv', n_envs=4)\n",
    "\n",
    "model = A2C(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_basketball\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_baselines.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "Analysis of the A2C Model results go here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
