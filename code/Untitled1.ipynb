{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Training-AI-Agents\" data-toc-modified-id=\"Training-AI-Agents-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Training AI Agents</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Imports</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stable-Baselines-Imports\" data-toc-modified-id=\"Stable-Baselines-Imports-1.0.1.1\"><span class=\"toc-item-num\">1.0.1.1&nbsp;&nbsp;</span>Stable Baselines Imports</a></span></li></ul></li><li><span><a href=\"#Creating-a-custom-Environment\" data-toc-modified-id=\"Creating-a-custom-Environment-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Creating a custom Environment</a></span></li><li><span><a href=\"#Checking-the-environment\" data-toc-modified-id=\"Checking-the-environment-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>Checking the environment</a></span></li><li><span><a href=\"#Random-Agent-Decisions\" data-toc-modified-id=\"Random-Agent-Decisions-1.0.4\"><span class=\"toc-item-num\">1.0.4&nbsp;&nbsp;</span>Random Agent Decisions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis:\" data-toc-modified-id=\"Analysis:-1.0.4.1\"><span class=\"toc-item-num\">1.0.4.1&nbsp;&nbsp;</span>Analysis:</a></span></li></ul></li><li><span><a href=\"#PPO---Promixal-Policy-Optimization\" data-toc-modified-id=\"PPO---Promixal-Policy-Optimization-1.0.5\"><span class=\"toc-item-num\">1.0.5&nbsp;&nbsp;</span>PPO - Promixal Policy Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis:\" data-toc-modified-id=\"Analysis:-1.0.5.1\"><span class=\"toc-item-num\">1.0.5.1&nbsp;&nbsp;</span>Analysis:</a></span></li></ul></li><li><span><a href=\"#PPO2---Promixal-Policy-Optimization\" data-toc-modified-id=\"PPO2---Promixal-Policy-Optimization-1.0.6\"><span class=\"toc-item-num\">1.0.6&nbsp;&nbsp;</span>PPO2 - Promixal Policy Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis:\" data-toc-modified-id=\"Analysis:-1.0.6.1\"><span class=\"toc-item-num\">1.0.6.1&nbsp;&nbsp;</span>Analysis:</a></span></li></ul></li><li><span><a href=\"#A2C---Advantage-Actor-Critic\" data-toc-modified-id=\"A2C---Advantage-Actor-Critic-1.0.7\"><span class=\"toc-item-num\">1.0.7&nbsp;&nbsp;</span>A2C - Advantage Actor Critic</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis:\" data-toc-modified-id=\"Analysis:-1.0.7.1\"><span class=\"toc-item-num\">1.0.7.1&nbsp;&nbsp;</span>Analysis:</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training AI Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "I am using Stable Baselines, and Open AI's gym to create and test the environment. I am also brining in player.py, that will provide the probabilities of an action happening, which are determined from NBA player statistics from the current NBA season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lots of help from Dustin Pierce at General Assembly\n",
    "#https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html\n",
    "#https://github.com/koulanurag/ma-gym/blob/master/ma_gym/envs/pong_duel/pong_duel.py\n",
    "#https://github.com/hardmaru/slimevolleygym/blob/master/slimevolleygym/slimevolley.py\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import random\n",
    "import time\n",
    "# import ball  --potential future add on\n",
    "import player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stable Baselines Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.env_checker import check_env\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import PPO1, PPO2, A2C, results_plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasketballEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where multiple agents learn strategies to put the ball in the hoop.\n",
    "    For this simple iteration, actions will be determined by probabilities rather than physics.\n",
    "    \"\"\"\n",
    "    # In google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "\n",
    "    \n",
    "    def __init__(self, step_cost=0, reward=0, max_rounds=1):\n",
    "        #Grid size will be standard basketball halfcourt at 6\"=1'-0\" scale\n",
    "        self._grid_shape = (100, 94)\n",
    "\n",
    "        #Number of players\n",
    "        self.n_agents = 4\n",
    "        self.n_agents_team_A = int(self.n_agents / 2)\n",
    "        self.n_agents_team_B = int(self.n_agents / 2)\n",
    "        self.reward = reward\n",
    "        self._max_rounds = max_rounds\n",
    "        self.action_space = spaces.MultiDiscrete([9, 2, 2])\n",
    "\n",
    "        self._step_count = None\n",
    "        self._step_cost = step_cost\n",
    "        self._total_episode_reward = None\n",
    "        self.agent_pos = {_: None for _ in range(self.n_agents)}\n",
    "        \n",
    "        #Set starting positions for agents in Team A\n",
    "        self.agent_pos[0] = (self._grid_shape[0]//2, self._grid_shape[1] - 2)\n",
    "        self.agent_pos[1] = (random.randint(0, self._grid_shape[0]//2), self._grid_shape[1] - 2)\n",
    "        \n",
    "        # Marking where a third player for Team A would go in the future\n",
    "        #         self.agent_pos[2] = (random.randint(self._grid_shape[0]//2, self._grid_shape[0]), self._grid_shape[1] - 2)\n",
    "        \n",
    "        #Set starting positions for agents in Team B\n",
    "        self.agent_pos[2] = (self.agent_pos[0], self._grid_shape[1] - 8)\n",
    "        self.agent_pos[3] = (self.agent_pos[1], self._grid_shape[1] - 8)\n",
    "\n",
    "        # Marking where a third player for Team B would go in the future        \n",
    "        #         self.agent_pos[5] = (random.randint(self._grid_shape[0]//2, self._grid_shape[0]), self._grid_shape[1] - 10)\n",
    "        self._agent_dones = None\n",
    "        self.__rounds = None\n",
    "\n",
    "        # Observing agent positions for 4 agents\n",
    "        self._obs_low = np.array([0. for _ in range(self.n_agents)])\n",
    "        self._obs_high = np.array([1. for _ in range(self.n_agents)])\n",
    "        self.observation_space = spaces.Box(low=self._obs_low, high=self._obs_high,\n",
    "                                        dtype=np.float32)\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "        \n",
    "    def is_done(self):\n",
    "        self.__rounds == self._max_rounds\n",
    "        \n",
    "    def get_action_meanings(self, agent_i=None):\n",
    "        if agent_i is not None:\n",
    "            assert agent_i <= self.n_agents\n",
    "            return [ACTION_MEANING[i] for i in range(self.action_space[agent_i].n)]\n",
    "        else:\n",
    "            return [[ACTION_MEANING[i] for i in range(ac.n)] for ac in self.action_space]\n",
    "\n",
    "    def __create_grid(self):\n",
    "        _grid = [[PRE_IDS['empty'] for _ in range(self._grid_shape[1])] for row in range(self._grid_shape[0])]\n",
    "        return _grid\n",
    "\n",
    "    def __update_agent_view(self, agent_i):\n",
    "        for row in range(self.agent_prev_pos[agent_i][0],\n",
    "                         self.agent_prev_pos[agent_i][0]):\n",
    "            self._full_obs[row][self.agent_prev_pos[agent_i][1]] = PRE_IDS['empty']\n",
    "\n",
    "        for row in range(self.agent_pos[agent_i][0], self.agent_pos[agent_i][0]):\n",
    "            self._full_obs[row][self.agent_pos[agent_i][1]] = PRE_IDS['agent'] + str(agent_i + 1) \\\n",
    "                                                              + '_' + str(row - self.agent_pos[agent_i][0])\n",
    "\n",
    "#     def __draw_base_img(self):\n",
    "#         self._base_img = draw.draw_grid(self._grid_shape[0], self._grid_shape[1],\n",
    "#                                    cell_size=CELL_SIZE, fill='white', line_color='white')\n",
    "\n",
    "    def __init_full_obs(self):\n",
    "        self._full_obs = self.__create_grid()\n",
    "        for agent_i in range(self.n_agents):\n",
    "            self.__update_agent_view(agent_i)\n",
    "\n",
    "        for agent_i in range(self.n_agents):\n",
    "            self.__update_agent_view(agent_i)\n",
    "\n",
    "#         self.__draw_base_img()\n",
    "\n",
    "    #Countdown timer as 24 second shot clock for each round\n",
    "    #https://www.geeksforgeeks.org/how-to-create-a-countdown-timer-using-python/\n",
    "    def countdown(self, t=24):     \n",
    "        while t: \n",
    "            mins, secs = divmod(t, 60) \n",
    "            timer = '{:02d}:{:02d}'.format(mins, secs)  \n",
    "            time.sleep(1) \n",
    "            t -= 1\n",
    "            return t \n",
    "\n",
    "    def get_agent_obs(self):\n",
    "        _obs = []\n",
    "\n",
    "        for agent_i in range(self.n_agents):\n",
    "            pos = self.agent_pos[agent_i]\n",
    "            _agent_i_obs = [pos[0] / self._grid_shape[0], pos[1] / self._grid_shape[1]]\n",
    "            \n",
    "            _obs.append(_agent_i_obs)\n",
    "\n",
    "        return _obs\n",
    "    \n",
    "##############\n",
    "#Define Reset#   \n",
    "##############\n",
    "\n",
    "    def reset(self):\n",
    "        self.__rounds = 0\n",
    "        self.countdown(24)\n",
    "        \n",
    "        #Set starting positions for agents in Team A\n",
    "        self.agent_pos[0] = (self._grid_shape[0]//2, self._grid_shape[1] - 2)\n",
    "        self.agent_pos[1] = (random.randint(0, self._grid_shape[0]//2), self._grid_shape[1] - 2)\n",
    "\n",
    "        \n",
    "        #Set starting positions for agents in Team B\n",
    "        self.agent_pos[2] = (self._grid_shape[0]//2, self._grid_shape[1] - 8)\n",
    "        self.agent_pos[3] = (random.randint(0, self._grid_shape[0]//2), self._grid_shape[1] - 8)\n",
    "\n",
    "        \n",
    "        self.agent_prev_pos = {_: self.agent_pos[_] for _ in range(self.n_agents)}\n",
    "        self._agent_dones = [False, False]\n",
    "        self.__init_full_obs()\n",
    "        self._step_count = 0\n",
    "        self._total_episode_reward = [0 for _ in range(self.n_agents)]\n",
    "\n",
    "        return np.array(self.get_agent_obs())\n",
    "\n",
    "    \n",
    "###############################\n",
    "#Define Properties and Actions#   \n",
    "###############################\n",
    "    \n",
    "            \n",
    "    #This will determine success of an action\n",
    "    def action_success(self, p_1):\n",
    "        return np.random.choice([0, 1], p=[1 - p_1, p_1])\n",
    "\n",
    "    #Determine if a player is close to the goal\n",
    "    def close_range(self):\n",
    "        for agent_i in n_agents:\n",
    "            if self.y > 0 and self.y < self._grid_shape[1] and self.x > 0 and self.x < self._grid_shape[0]:\n",
    "                if np.sqrt((x.self - GOAL[0])**2 + (y.self-GOAL[1])**2) <= 6:\n",
    "                    agent_i.player._close_range = True\n",
    "\n",
    "    #Determine if a player is mid-range from the goal\n",
    "    def midrange(self):\n",
    "        for agent_i in n_agents:\n",
    "            if self.y > 0 and self.y < self._grid_shape[1] and self.x > 0 and self.x < self._grid_shape[0]:\n",
    "                if player._close_range == False and player._three_point_range == False:\n",
    "                    agent_i.player._midrange = True\n",
    "\n",
    "    #Determine if a player is in three point range\n",
    "    def _three_point_range(self):\n",
    "        for agent_i in n_agents:\n",
    "            if self.y > 0 and self.y < self._grid_shape[1] and self.x > 0 and self.x < self._grid_shape[0]:\n",
    "                if self.y <= 19.67 and self.x <= 6.67 or self.x >= 93.33:\n",
    "                    player._three_point_range = True\n",
    "                elif self.y > 19.67 and np.sqrt((x.self - GOAL[0])**2 + (y.self-GOAL[1])**2) > 44.3:\n",
    "                    agent_i.player._three_point_range = True\n",
    "\n",
    "    #define defensive rebound, will be a reward for the defensive team\n",
    "    def d_rebound(self):\n",
    "        return True\n",
    "        \n",
    "    def rebound(self):\n",
    "    for agent_i in n_agents_team_A:\n",
    "        if action_success(0.3):\n",
    "            o_rebounder = random.choice([agent_j in n_agents_team_A])\n",
    "            o_rebounder.player._has_ball = True\n",
    "        else:\n",
    "            return d_rebound()\n",
    "\n",
    "    #define shot, a made shot will be a reward for the offensive team\n",
    "    def shot(self):\n",
    "        \n",
    "        for agent_i in self.n_agents:\n",
    "\n",
    "            #Can only shoot if the player has the ball\n",
    "            if player._has_ball():\n",
    "\n",
    "                #Close range shot\n",
    "                if player._close_range():\n",
    "                    shot = action_success(player.shooting_close)\n",
    "                    if shot == 1:\n",
    "                        return 2\n",
    "                    else:\n",
    "                        rebound()\n",
    "\n",
    "                #Midrange shot\n",
    "                if player._midrange():\n",
    "                    shot = action_success(player.shooting_midrange)\n",
    "                    if shot == 1:\n",
    "                        return 2\n",
    "                    else:\n",
    "                        rebound()\n",
    "\n",
    "\n",
    "                #3 point shot\n",
    "                if _three_point_range():\n",
    "                    shot = action_success(player.shooting3pts)\n",
    "                    if shot == 1:\n",
    "                        return 3\n",
    "                    else:\n",
    "                        rebound()\n",
    "                        \n",
    "    def ball_pass(self):\n",
    "        for agent_i, agent_j in n_agents_team_A:\n",
    "            if agent_i.player._has_ball():\n",
    "                agent_j.player._has_ball = True\n",
    "                agent_i.player._has_ball = False\n",
    "            else:\n",
    "                agent_i.player._has_ball = True\n",
    "                agent_j.player._has_ball = False\n",
    "\n",
    "    def defended(self):\n",
    "        for agent_i in n_agents_team_B:\n",
    "            for agent_j in n_agents_team_A:\n",
    "                if np.sqrt(agent_i.self.x**2 + agent_i.self.y**2) < 5:\n",
    "                    agent_j.player._is_defended=True\n",
    "                else:\n",
    "                    agent_j.player._is_defended=False\n",
    "    \n",
    "    def steal(self):\n",
    "        for agent_i in n_agents_team_B:\n",
    "            for agent_j in n_agents_team_A:\n",
    "                if agent_j.player._has_ball and agent_j.player._is_defended:\n",
    "                    return action_success(0.02)\n",
    "    \n",
    "    def block(self):\n",
    "        for agent_i in n_agents_team_B:\n",
    "            for agent_j in n_agents_team_A:\n",
    "                if agent_j.player._has_ball and agent_j.player._is_defended and agent_j.player._close_range:\n",
    "                    return action_success(0.04)\n",
    "                if agent_j.player._has_ball and agent_j.player._is_defended and agent_j.player._midrange:\n",
    "                    return action_success(0.03)\n",
    "                if agent_j.player._has_ball and agent_j.player._defended and agent_j.player._three_point_range:\n",
    "                    return action_success(0.02)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "###############\n",
    "#Define Render#   \n",
    "###############\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        img = copy.copy(self._base_img)\n",
    "        for agent_i in range(self.n_agents):\n",
    "            for row in self.agent_pos[agent_i][0]:\n",
    "                fill_cell(img, (row, self.agent_pos[agent_i][1]), cell_size=CELL_SIZE, fill=AGENT_COLORS[agent_i])\n",
    "\n",
    "        img = draw_border(img, border_width=2, fill='gray')\n",
    "\n",
    "        img = np.asarray(img)\n",
    "        if mode == 'rgb_array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            from gym.envs.classic_control import rendering\n",
    "            if self.viewer is None:\n",
    "                self.viewer = rendering.SimpleImageViewer()\n",
    "            self.viewer.imshow(img)\n",
    "            return self.viewer.isopen\n",
    "\n",
    "    def __update_agent_pos(self, agent_i, move):\n",
    "\n",
    "        curr_pos = copy.copy(self.agent_pos[agent_i])\n",
    "        if self.x > 0 and self.x < self._grid_shape[0] and self.y > 0 and self.y < self._grid_shape[1]:\n",
    "            if move == 0:  # noop\n",
    "                next_pos = None\n",
    "            elif move == 1:  # up\n",
    "                next_pos = [curr_pos[0] - 1, curr_pos[1]]\n",
    "            elif move == 2:  # upright\n",
    "                next_pos = [curr_pos[0] - 1, curr_pos[1] + 1]\n",
    "            elif move == 3:  # right\n",
    "                next_pos = [curr_pos[0], curr_pos[1] + 1]\n",
    "            elif move == 4:  # downright\n",
    "                next_pos = [curr_pos[0] + 1, curr_pos[1] + 1]\n",
    "            elif move == 5:  # down\n",
    "                next_pos = [curr_pos[0] + 1, curr_pos[1]]\n",
    "            elif move == 6:  # downleft\n",
    "                next_pos = [curr_pos[0] + 1, curr_pos[1] - 1]\n",
    "            elif move == 7:  # left\n",
    "                next_pos = [curr_pos[0], curr_pos[1] - 1]\n",
    "            elif move == 8:  # upleft\n",
    "                next_pos = [curr_pos[0] - 1, curr_pos[1] - 1]\n",
    "            else:\n",
    "                raise Exception('Action Not found!')\n",
    "        else:\n",
    "            raise Exception('Out of Bounds!')\n",
    "\n",
    "        if next_pos is not None:\n",
    "            self.agent_prev_pos[agent_i] = self.agent_pos[agent_i]\n",
    "            self.agent_pos[agent_i] = next_pos\n",
    "            self.__update_agent_view(agent_i)\n",
    "\n",
    "#############\n",
    "#Define Seed#   \n",
    "#############\n",
    "\n",
    "    def seed(self, n=None):\n",
    "        self.np_random, seed = seeding.np_random(n)\n",
    "        return [seed]\n",
    "\n",
    "#############\n",
    "#Define Step#   \n",
    "#############\n",
    "    \n",
    "    def step(self, action_n):\n",
    "        assert len(action_n) == self.n_agents\n",
    "        self._step_count += 1\n",
    "        rewards = [self._step_cost for _ in range(self.n_agents)]\n",
    "\n",
    "        # if shot made, new round\n",
    "        if shot() == 2:\n",
    "            rewards = [2, 0]\n",
    "            self.__rounds += 1\n",
    "            \n",
    "        elif shot() == 3:\n",
    "            rewards = [3, 0]\n",
    "            self.__rounds += 1\n",
    "\n",
    "        # if steal made, new round\n",
    "        if steal():\n",
    "            rewards = [0, 2]\n",
    "            self.__rounds += 1\n",
    "        \n",
    "        # if defensive rebound made, new round\n",
    "        if d_rebound():\n",
    "            rewards = [0, 1]\n",
    "            self.__rounds += 1\n",
    "        \n",
    "        # if block made, new round\n",
    "        if block():\n",
    "            rewards = [0, 2]\n",
    "            self.__rounds += 1\n",
    "            \n",
    "        # if Offense fails to get off a shot within time limit\n",
    "        if countdown() < 1:\n",
    "            rewards = [0, 2]\n",
    "            self.__rounds += 1\n",
    "                        \n",
    "        if self.__rounds == self._max_rounds:\n",
    "            self._agent_dones = [True for _ in range(self.n_agents)]\n",
    "        else:\n",
    "            for agent_i in range(self.n_agents):\n",
    "                self.__update_agent_pos(agent_i, action_n[agent_i])\n",
    "\n",
    "        for i in range(self.n_agents):\n",
    "            self._total_episode_reward[i] += rewards[i]\n",
    "\n",
    "        return self.get_agent_obs(), rewards, self._agent_dones, {'rounds': self.__rounds}\n",
    "\n",
    "# Define constants for clearer code\n",
    "\n",
    "CELL_SIZE = 5\n",
    "\n",
    "#Goal Location\n",
    "GOAL = [50, 10.5]\n",
    "\n",
    "ACTION_MEANING = {\n",
    "    0 : 'NOOP',\n",
    "    1 : 'UP',\n",
    "    2 : 'UPRIGHT',\n",
    "    3 : 'RIGHT',\n",
    "    4 : 'DOWNRIGHT',\n",
    "    5 : 'DOWN',\n",
    "    6 : 'DOWNLEFT',\n",
    "    7 : 'LEFT',\n",
    "    8 : 'UPLEFT',\n",
    "    9 : 'BALL_PASS',\n",
    "    10 : 'SHOOT',\n",
    "    11 : 'STEAL',\n",
    "    12 : 'BLOCK',\n",
    "}\n",
    "\n",
    "AGENT_TEAMS = {\n",
    "    0: 'A',\n",
    "    1: 'A',\n",
    "    2: 'B',\n",
    "    3: 'B',\n",
    "}\n",
    "\n",
    "AGENT_COLORS = {\n",
    "    0: 'red',\n",
    "    1: 'red',\n",
    "    2: 'blue',\n",
    "    3: 'blue',\n",
    "}\n",
    "\n",
    "WALL_COLOR = 'black'\n",
    "\n",
    "# each pre-id should be unique and single char\n",
    "PRE_IDS = {\n",
    "    'agent': 'A',\n",
    "    'goal' : 'G',\n",
    "    'empty': 'O'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The observation returned by the `reset()` method does not match the given observation space",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-50e46f9d950d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBasketballEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# It will check your custom environment and output additional warnings if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines\\common\\env_checker.py\u001b[0m in \u001b[0;36mcheck_env\u001b[1;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;31m# ============ Check the returned values ===============\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[0m_check_returned_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;31m# ==== Check the render method and the declared render modes ====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines\\common\\env_checker.py\u001b[0m in \u001b[0;36m_check_returned_values\u001b[1;34m(env, observation_space, action_space)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0m_check_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m# Sample a random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines\\common\\env_checker.py\u001b[0m in \u001b[0;36m_check_obs\u001b[1;34m(obs, observation_space, method_name)\u001b[0m\n\u001b[0;32m     86\u001b[0m                                              \"method must be a numpy array\".format(method_name))\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     assert observation_space.contains(obs), (\"The observation returned by the `{}()` \"\n\u001b[0m\u001b[0;32m     89\u001b[0m                                              \"method does not match the given observation space\".format(method_name))\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: The observation returned by the `reset()` method does not match the given observation space"
     ]
    }
   ],
   "source": [
    "env = BasketballEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "    def step(self, env):\n",
    "        # current_obs = env.get_observation()\n",
    "        actions = ACTION_MEANING\n",
    "        action = random.choice(actions)\n",
    "        reward = env.step(action)\n",
    "        #print(f\"Took action {action} and got reward {reward}\")\n",
    "        self.total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-40002cb80bf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Total reward: {agent.total_reward}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-105-813882c8a18d>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mACTION_MEANING\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;31m#print(f\"Took action {action} and got reward {reward}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-113-b5d669cacf8a>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action_n)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_cost\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = BasketballEnv()\n",
    "agent = RandomAgent()\n",
    "while not env.is_done():\n",
    "    agent.step(env)\n",
    "print(f\"Total reward: {agent.total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "Analysis of the random agent results go here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO - Promixal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BasketballEnv')\n",
    "\n",
    "model = PPO1(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo1_Basketball\")\n",
    "\n",
    "model = PPO1.load(\"ppo1_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_baselines.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "Analysis of the PPO Model results go here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO2 - Promixal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocess environment\n",
    "env = make_vec_env('BasketballEnv', n_envs=4)\n",
    "\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo2_basketball\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO2.load(\"ppo2_basketball\")\n",
    "\n",
    "# Enjoy trained agent\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_baselines.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "Analysis of the PPO2 Model results go here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C - Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stable-baselines.readthedocs.io/en/master/modules/a2c.html\n",
    "# Parallel environments\n",
    "env = make_vec_env('BasketballEnv', n_envs=4)\n",
    "\n",
    "model = A2C(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_basketball\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_baselines.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "Analysis of the A2C Model results go here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        \n",
    "        \n",
    "    def _get_obs(self, obs):\n",
    "        \"\"\"\n",
    "        Concatenate the time feature to the current observation.\n",
    "        :param obs: (np.ndarray)\n",
    "        :return: (np.ndarray)\n",
    "        \"\"\"\n",
    "        # Remaining time is more general\n",
    "        time_feature = 1 - (self._current_step / self._max_steps)\n",
    "        if self._test_mode:\n",
    "            time_feature = 1.0\n",
    "        # Optionnaly: concatenate [time_feature, time_feature ** 2]\n",
    "    return np.concatenate((obs, [time_feature]))\n",
    "\n",
    "    def reset(self):\n",
    "    \"\"\"\n",
    "    Important: the observation must be a numpy array\n",
    "    :return: (np.array) \n",
    "    \"\"\"\n",
    "    # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = self.grid_size - 1\n",
    "   \n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "    return np.array([self.agent_pos]).astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "    # Account for the boundaries of the grid\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "    # Are we at the left of the grid?\n",
    "    done = bool(self.agent_pos == 0)\n",
    "\n",
    "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "    reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        if mode != 'console':\n",
    "            raise NotImplementedError()\n",
    "            # agent is represented as a cross, rest as a dot\n",
    "            print(\".\" * self.agent_pos, end=\"\")\n",
    "            print(\"x\", end=\"\")\n",
    "            print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lots of help from Dustin Pierce\n",
    "class Environment:\n",
    "    SIZE = [50, 94]\n",
    "    GOAL_A = [10.5, 50]\n",
    "    BACKBOARD_A = [25, 4]\n",
    "    BACKBOARD_B = [25, 90]\n",
    "    GOAL_B = [25, 88.75]\n",
    "    3PT_LINE = [Coods] \n",
    "    SCORE = 0\n",
    "    \n",
    "    def shot(self):\n",
    "        if self.x ** 2 + self.y**2 > AMOUNT:\n",
    "            \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.time_left = countdown(300)\n",
    "\n",
    "        self.x = 44\n",
    "        self.y = 25\n",
    "        \n",
    "    #Countdown timer as game clock\n",
    "    #https://www.geeksforgeeks.org/how-to-create-a-countdown-timer-using-python/\n",
    "    def countdown(t=300):     \n",
    "        while t: \n",
    "            mins, secs = divmod(t, 60) \n",
    "            timer = '{:02d}:{:02d}'.format(mins, secs)  \n",
    "            time.sleep(1) \n",
    "            t -= 1\n",
    "            return t \n",
    "\n",
    "    def num_states(self):\n",
    "        return t\n",
    "\n",
    "    def num_actions(self):\n",
    "        return 10\n",
    "\n",
    "    def get_observation(self):\n",
    "        return [self.x, self.y]\n",
    "\n",
    "    def get_state_num(self):\n",
    "        return self.x*self.SIZE + self.y\n",
    "\n",
    "    def get_pos_from_state_num(self, state_num):\n",
    "        return (state_num // self.SIZE, state_num % self.SIZE)\n",
    "\n",
    "    def has_ball(self):\n",
    "        if self.x == ball.x and self.y == ball.y:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def has_dribble:\n",
    "        \n",
    "    def on_offense(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_actions(self):\n",
    "        if has_ball == False:\n",
    "            return [\"up\", \"up-right\", \"right\", \"down-right\", \"down\", \"down-left\", \"left\", \"up-left\", \"jump\", \"screen\"]\n",
    "        if has_ball == True and has_dribble == True:\n",
    "            return ['pass', 'shoot', 'up', 'down', 'left', 'right']\n",
    "\n",
    "    def is_done(self):\n",
    "        return self.time_left == 0\n",
    "\n",
    "    def at_goal(self):\n",
    "        return self.x == self.GOAL[0] and self.y == self.GOAL[1]\n",
    "\n",
    "    def is_clear(self, x, y):\n",
    "        for w in self.WALLS:\n",
    "            if x == w[0] and y == w[1]:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def action(self, action):\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Episode is already over\")\n",
    "        self.steps_left -= 1\n",
    "        if action == \"up\" and self.y > 0:\n",
    "            if self.is_clear(self.x, self.y-1):\n",
    "            self.y -= 1\n",
    "        elif action == \"down\" and self.y < self.SIZE-1:\n",
    "            if self.is_clear(self.x, self.y+1):\n",
    "            self.y += 1\n",
    "        elif action == \"left\" and self.x > 0:\n",
    "            if self.is_clear(self.x-1, self.y):\n",
    "            self.x -= 1\n",
    "        elif action == \"right\" and self.x < self.SIZE-1:\n",
    "            if self.is_clear(self.x+1, self.y):\n",
    "            self.x += 1\n",
    "\n",
    "        if self.x == self.GOAL[0] and self.y == self.GOAL[1]:\n",
    "            return 1.0\n",
    "        return 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
